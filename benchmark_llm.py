#!/usr/bin/env python3
"""Î≤îÏö© LLM Ï∂îÎ°† ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨ Ïä§ÌÅ¨Î¶ΩÌä∏.

Ïù¥ Î™®ÎìàÏùÄ vLLMÍ≥º TensorRT-LLM Ï∂îÎ°† ÏóîÏßÑÏùò ÏÑ±Îä•ÏùÑ Ï∏°Ï†ïÌïòÍ≥† ÎπÑÍµêÌïòÍ∏∞ ÏúÑÌïú
Î≤§ÏπòÎßàÌÅ¨ ÎèÑÍµ¨ÏûÖÎãàÎã§. OpenAI API Ìò∏Ìôò ÏóîÎìúÌè¨Ïù∏Ìä∏Î•º ÌÜµÌï¥ Îëê ÏóîÏßÑÏùò ÏÑ±Îä•ÏùÑ
ÎèôÏùºÌïú Ï°∞Í±¥ÏóêÏÑú Ï∏°Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§.

Ï£ºÏöî Í∏∞Îä•:
    - OpenAI API Ìò∏Ìôò ÏóîÎìúÌè¨Ïù∏Ìä∏ ÌÖåÏä§ÌåÖ (/v1/completions)
    - ÏõåÎ∞çÏóÖ(warm-up) Îã®Í≥ÑÎ•º ÌÜµÌïú Ï†ïÌôïÌïú ÏÑ±Îä• Ï∏°Ï†ï
    - ÏßÄÏó∞ÏãúÍ∞Ñ(latency) Î∞è Ï≤òÎ¶¨Îüâ(throughput) ÌÜµÍ≥Ñ ÏàòÏßë
    - Î∞±Î∂ÑÏúÑÏàò(percentile) Í∏∞Î∞ò ÏÉÅÏÑ∏ ÏÑ±Îä• Î∂ÑÏÑù
    - JSON ÌòïÏãùÏùò Í≤∞Í≥º Ï†ÄÏû• Î∞è ÎπÑÍµê ÏßÄÏõê

Î≤§ÏπòÎßàÌÅ¨ Î©îÌä∏Î¶≠:
    - ÏßÄÏó∞ÏãúÍ∞Ñ: ÏöîÏ≤≠Î∂ÄÌÑ∞ ÏùëÎãµÍπåÏßÄÏùò ÏãúÍ∞Ñ (ÌèâÍ∑†, Ï§ëÏïôÍ∞í, min/max, p50/p90/p95/p99)
    - Ï≤òÎ¶¨Îüâ: Ï¥àÎãπ ÏÉùÏÑ± ÌÜ†ÌÅ∞ Ïàò (tokens/s)
    - ÏÑ±Í≥µÎ•†: Ï†ÑÏ≤¥ ÏöîÏ≤≠ ÎåÄÎπÑ ÏÑ±Í≥µÌïú ÏöîÏ≤≠ ÎπÑÏú®
    - ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ: ÌîÑÎ°¨ÌîÑÌä∏ ÌÜ†ÌÅ∞, ÏÉùÏÑ± ÌÜ†ÌÅ∞ ÌÜµÍ≥Ñ

ÏÇ¨Ïö© ÏòàÏãú:
    # Í∏∞Î≥∏ Ïã§Ìñâ (ÌôòÍ≤ΩÎ≥ÄÏàò ÏóÜÏù¥)
    python3 benchmark_llm.py

    # vLLM Î≤§ÏπòÎßàÌÅ¨ (Ìè¨Ìä∏ 8000)
    BENCHMARK_PORT=8000 BENCHMARK_OUTPUT=results_vllm.json python3 benchmark_llm.py

    # TensorRT-LLM Î≤§ÏπòÎßàÌÅ¨ (Ìè¨Ìä∏ 8001, ÏõåÎ∞çÏóÖ 5Ìöå)
    BENCHMARK_PORT=8001 BENCHMARK_OUTPUT=results_tensorrt.json BENCHMARK_WARMUP=5 python3 benchmark_llm.py

    # Ïª§Ïä§ÌÖÄ URL ÏÇ¨Ïö©
    BENCHMARK_URL=http://custom-server:9000/v1/completions python3 benchmark_llm.py

ÌôòÍ≤Ω Î≥ÄÏàò:
    BENCHMARK_PORT: API ÏÑúÎ≤Ñ Ìè¨Ìä∏ (Í∏∞Î≥∏Í∞í: 8001)
    BENCHMARK_URL: Ï†ÑÏ≤¥ API URL (ÏßÄÏ†ï Ïãú BENCHMARK_PORT Î¨¥Ïãú)
    BENCHMARK_OUTPUT: Í≤∞Í≥º Ï†ÄÏû• ÌååÏùºÎ™Ö (Í∏∞Î≥∏Í∞í: results_tensorrt.json)
    BENCHMARK_WARMUP: ÏõåÎ∞çÏóÖ ÏöîÏ≤≠ ÌöüÏàò (Í∏∞Î≥∏Í∞í: 3)

ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï:
    - Î™®Îç∏: Qwen3-8B
    - ÏöîÏ≤≠ ÌöüÏàò: 10Ìöå (ÏõåÎ∞çÏóÖ Ï†úÏô∏)
    - ÏûÖÎ†• Í∏∏Ïù¥: ~128 ÌÜ†ÌÅ∞
    - Ï∂úÎ†• Í∏∏Ïù¥: 128 ÌÜ†ÌÅ∞
    - Temperature: 0.7

ÏõåÎ∞çÏóÖ Îã®Í≥Ñ:
    Ï≤´ Î™á Î≤àÏùò Ï∂îÎ°† ÏöîÏ≤≠ÏùÄ Î™®Îç∏ Î°úÎî©, Ï∫êÏãú Ï¥àÍ∏∞Ìôî Îì±ÏúºÎ°ú Ïù∏Ìï¥
    Ïã§Ï†ú ÏÑ±Îä•Î≥¥Îã§ ÎäêÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§. ÏõåÎ∞çÏóÖ Îã®Í≥ÑÎäî Ïù¥Îü¨Ìïú Ï¥àÍ∏∞Ìôî
    Ïò§Î≤ÑÌó§ÎìúÎ•º Ï†úÍ±∞ÌïòÏó¨ Ï†ïÌôïÌïú ÏÑ±Îä• Ï∏°Ï†ïÏùÑ Í∞ÄÎä•ÌïòÍ≤å Ìï©ÎãàÎã§.

Ï∂úÎ†• ÌòïÏãù:
    Ïã§Ìñâ Ï§ë ÏßÑÌñâ ÏÉÅÌô©ÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú Ï∂úÎ†•ÌïòÎ©∞, ÏôÑÎ£å ÌõÑ Îã§ÏùåÏùÑ Ìè¨Ìï®Ìïú
    ÏÉÅÏÑ∏Ìïú ÌÜµÍ≥ÑÎ•º ÌëúÏãúÌï©ÎãàÎã§:
    - ÏÑ±Í≥µ/Ïã§Ìå® ÏöîÏ≤≠ Ïàò
    - ÏßÄÏó∞ÏãúÍ∞Ñ ÌÜµÍ≥Ñ (ÌèâÍ∑†, Ï§ëÏïôÍ∞í, min/max, Î∞±Î∂ÑÏúÑÏàò)
    - Ï≤òÎ¶¨Îüâ ÌÜµÍ≥Ñ (ÌèâÍ∑†, Ï§ëÏïôÍ∞í, min/max)
    - ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ
    - JSON ÌååÏùºÎ°ú Ï†ÄÏû•Îêú ÏÉÅÏÑ∏ Í≤∞Í≥º

Ï∞∏Í≥†:
    - Î≤§ÏπòÎßàÌÅ¨ Ïã§Ìñâ Ï†Ñ ÎåÄÏÉÅ ÏÑúÎ≤ÑÍ∞Ä Ïã§Ìñâ Ï§ëÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§
    - MakefileÏùò make benchmark Î™ÖÎ†πÏúºÎ°ú ÏûêÎèôÌôîÎêú Î≤§ÏπòÎßàÌÅ¨ Í∞ÄÎä•
    - Í≤∞Í≥º ÌååÏùºÏùÄ make compare Î™ÖÎ†πÏúºÎ°ú ÎπÑÍµê Í∞ÄÎä•
"""
import json
import time
import sys
import os
from statistics import mean, median
import requests

# Test configuration (matching vLLM benchmark)
# API_URL can be overridden by environment variable or command line argument
DEFAULT_PORT = os.environ.get("BENCHMARK_PORT", "8001")
API_URL = os.environ.get("BENCHMARK_URL", f"http://localhost:{DEFAULT_PORT}/v1/completions")
MODEL = "/models/Qwen3-8B"
NUM_REQUESTS = 10
NUM_WARMUP = int(os.environ.get("BENCHMARK_WARMUP", "3"))  # Number of warm-up requests
INPUT_LENGTH = 128
OUTPUT_LENGTH = 128
TEMPERATURE = 0.7

# Test prompt (same as vLLM)
PROMPT = " ".join(["machine learning", "artificial intelligence", "neural network",
                   "deep learning", "computer vision", "natural language processing",
                   "model training", "inference", "optimization", "performance", "benchmark"] * 20)

def run_inference(prompt: str, max_tokens: int) -> dict:
    """Îã®Ïùº Ï∂îÎ°† ÏöîÏ≤≠ÏùÑ Ïã§ÌñâÌïòÍ≥† ÏÑ±Îä• Î©îÌä∏Î¶≠ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.

    Ïù¥ Ìï®ÏàòÎäî OpenAI API Ìò∏Ìôò ÏóîÎìúÌè¨Ïù∏Ìä∏Î°ú POST ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Í≥†,
    ÏùëÎãµ ÏãúÍ∞ÑÍ≥º ÏÉùÏÑ±Îêú ÌÜ†ÌÅ∞ ÏàòÎ•º Ï∏°Ï†ïÌï©ÎãàÎã§. ÏöîÏ≤≠ ÏãúÏûëÎ∂ÄÌÑ∞ ÏùëÎãµ ÏàòÏã†ÍπåÏßÄÏùò
    Ï†ÑÏ≤¥ ÏãúÍ∞ÑÏùÑ ÏßÄÏó∞ÏãúÍ∞Ñ(latency)ÏúºÎ°ú Ï∏°Ï†ïÌïòÎ©∞, Ïù¥Î•º ÌÜµÌï¥ Ï≤òÎ¶¨Îüâ(throughput)ÏùÑ
    Í≥ÑÏÇ∞Ìï©ÎãàÎã§.

    Args:
        prompt: ÏôÑÏÑ±(completion)ÏùÑ ÏÉùÏÑ±Ìï† ÏûÖÎ†• ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏.
            ÏùºÎ∞òÏ†ÅÏúºÎ°ú Í∏∞Ïà† Ïö©Ïñ¥Î•º Î∞òÎ≥µÌïú Í∏¥ ÌÖçÏä§Ìä∏Î•º ÏÇ¨Ïö©ÌïòÏó¨
            ÏùºÍ¥ÄÎêú ÌÜ†ÌÅ∞ Í∏∏Ïù¥Î•º Ïú†ÏßÄÌï©ÎãàÎã§.
        max_tokens: ÏÉùÏÑ±Ìï† ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò. Ï∂úÎ†• Í∏∏Ïù¥Î•º Ï†úÏñ¥ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.
            Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù Îçî Í∏¥ ÏùëÎãµÏù¥ ÏÉùÏÑ±ÎêòÏßÄÎßå Ï≤òÎ¶¨ ÏãúÍ∞ÑÎèÑ Ï¶ùÍ∞ÄÌï©ÎãàÎã§.

    Returns:
        dict: Ï∂îÎ°† Í≤∞Í≥ºÎ•º Ìè¨Ìï®ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨. Îã§Ïùå ÌÇ§Î•º Ìè¨Ìï®Ìï©ÎãàÎã§:
            - success (bool): ÏöîÏ≤≠ ÏÑ±Í≥µ Ïó¨Î∂Ä
            - latency (float): ÏöîÏ≤≠Î∂ÄÌÑ∞ ÏùëÎãµÍπåÏßÄÏùò ÏãúÍ∞Ñ (Ï¥à Îã®ÏúÑ)
            - prompt_tokens (int): ÏûÖÎ†• ÌîÑÎ°¨ÌîÑÌä∏Ïùò ÌÜ†ÌÅ∞ Ïàò (ÏÑ±Í≥µ Ïãú)
            - completion_tokens (int): ÏÉùÏÑ±Îêú Ï∂úÎ†•Ïùò ÌÜ†ÌÅ∞ Ïàò (ÏÑ±Í≥µ Ïãú)
            - total_tokens (int): Ï¥ù ÌÜ†ÌÅ∞ Ïàò (prompt + completion, ÏÑ±Í≥µ Ïãú)
            - tokens_per_second (float): Ï¥àÎãπ ÏÉùÏÑ±Îêú ÌÜ†ÌÅ∞ Ïàò (ÏÑ±Í≥µ Ïãú)
            - response (str): ÏÉùÏÑ±Îêú ÌÖçÏä§Ìä∏ ÏùëÎãµ (ÏÑ±Í≥µ Ïãú)
            - error (str): Ïò§Î•ò Î©îÏãúÏßÄ (Ïã§Ìå® Ïãú)

    ÏòàÏãú:
        >>> result = run_inference("What is machine learning?", 50)
        >>> if result["success"]:
        ...     print(f"Latency: {result['latency']:.2f}s")
        ...     print(f"Throughput: {result['tokens_per_second']:.2f} tokens/s")
        ...     print(f"Generated: {result['completion_tokens']} tokens")
        Latency: 1.23s
        Throughput: 40.65 tokens/s
        Generated: 50 tokens

        >>> # Ïã§Ìå®Ìïú ÏöîÏ≤≠Ïùò Í≤ΩÏö∞
        >>> failed_result = run_inference("test", 100)
        >>> if not failed_result["success"]:
        ...     print(f"Error: {failed_result['error']}")
        Error: Connection refused

    Ï∞∏Í≥†:
        - ÌÉÄÏûÑÏïÑÏõÉÏùÄ 60Ï¥àÎ°ú ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏäµÎãàÎã§
        - HTTP ÏÉÅÌÉú ÏΩîÎìúÍ∞Ä 200Ïù¥ ÏïÑÎãàÎ©¥ Ïã§Ìå®Î°ú Ï≤òÎ¶¨Îê©ÎãàÎã§
        - ÏßÄÏó∞ÏãúÍ∞ÑÏù¥ 0Ïù∏ Í≤ΩÏö∞ tokens_per_secondÎäî 0ÏúºÎ°ú Í≥ÑÏÇ∞Îê©ÎãàÎã§
        - ÏùëÎãµ Î≥∏Î¨∏ÏùÄ OpenAI API ÌòïÏãùÏùÑ Îî∞ÎùºÏïº Ìï©ÎãàÎã§ (choices, usage ÌïÑÎìú Ìè¨Ìï®)
    """
    payload = {
        "model": MODEL,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": TEMPERATURE,
        "stream": False
    }

    start_time = time.time()
    response = requests.post(API_URL, json=payload, timeout=60)
    latency = time.time() - start_time

    if response.status_code != 200:
        return {
            "success": False,
            "error": response.text,
            "latency": latency
        }

    data = response.json()
    choice = data["choices"][0]
    usage = data["usage"]

    return {
        "success": True,
        "latency": latency,
        "prompt_tokens": usage.get("prompt_tokens", 0),
        "completion_tokens": usage.get("completion_tokens", 0),
        "total_tokens": usage.get("total_tokens", 0),
        "tokens_per_second": usage.get("completion_tokens", 0) / latency if latency > 0 else 0,
        "response": choice["text"]
    }

def main():
    """Î≤§ÏπòÎßàÌÅ¨ Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò.

    Ïù¥ Ìï®ÏàòÎäî Ï†ÑÏ≤¥ Î≤§ÏπòÎßàÌÅ¨ ÌîÑÎ°úÏÑ∏Ïä§Î•º Í¥ÄÎ¶¨ÌïòÍ≥† Ïã§ÌñâÌï©ÎãàÎã§.
    ÏõåÎ∞çÏóÖ Îã®Í≥Ñ, Ïã§Ï†ú Î≤§ÏπòÎßàÌÅ¨ Ïã§Ìñâ, ÌÜµÍ≥Ñ Í≥ÑÏÇ∞, Í≤∞Í≥º Ï∂úÎ†• Î∞è Ï†ÄÏû•Ïùò
    Ï†ÑÏ≤¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏàòÌñâÌï©ÎãàÎã§.

    Ïã§Ìñâ Îã®Í≥Ñ:
        1. ÏÑ§Ï†ï Ï†ïÎ≥¥ Ï∂úÎ†•: API URL, Î™®Îç∏, ÏöîÏ≤≠ ÌöüÏàò Îì±
        2. ÏõåÎ∞çÏóÖ Îã®Í≥Ñ: NUM_WARMUP ÌöüÏàòÎßåÌÅº ÏÇ¨Ï†Ñ Ï∂îÎ°† Ïã§Ìñâ
        3. Î≤§ÏπòÎßàÌÅ¨ Îã®Í≥Ñ: NUM_REQUESTS ÌöüÏàòÎßåÌÅº Ï∏°Ï†ïÏö© Ï∂îÎ°† Ïã§Ìñâ
        4. ÌÜµÍ≥Ñ Í≥ÑÏÇ∞: ÏßÄÏó∞ÏãúÍ∞Ñ, Ï≤òÎ¶¨Îüâ, ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ ÏÇ∞Ï∂ú
        5. Í≤∞Í≥º Ï∂úÎ†•: ÏΩòÏÜîÏóê ÏÉÅÏÑ∏Ìïú ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÌëúÏãú
        6. Í≤∞Í≥º Ï†ÄÏû•: JSON ÌååÏùºÎ°ú ÏÉÅÏÑ∏ Í≤∞Í≥º Ï†ÄÏû•

    ÏõåÎ∞çÏóÖ Îã®Í≥ÑÏùò Ï§ëÏöîÏÑ±:
        Ï≤´ Î™á Î≤àÏùò Ï∂îÎ°†ÏùÄ Îã§Ïùå Ïù¥Ïú†Î°ú Ïã§Ï†ú ÏÑ±Îä•Î≥¥Îã§ ÎäêÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§:
        - Î™®Îç∏ Í∞ÄÏ§ëÏπòÏùò GPU Î©îÎ™®Î¶¨ Î°úÎî©
        - CUDA Ïª§ÎÑê Ï¥àÍ∏∞Ìôî Î∞è Ïª¥ÌååÏùº
        - KV Ï∫êÏãú Î©îÎ™®Î¶¨ Ìï†Îãπ
        - Î∞∞Ïπò Ï≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî
        ÏõåÎ∞çÏóÖ Îã®Í≥ÑÎäî Ïù¥Îü¨Ìïú Ï¥àÍ∏∞Ìôî Ïò§Î≤ÑÌó§ÎìúÎ•º Ï†úÍ±∞ÌïòÏó¨
        ÏàúÏàòÌïú Ï∂îÎ°† ÏÑ±Îä•ÏùÑ Ï∏°Ï†ïÌï† Ïàò ÏûàÍ≤å Ìï©ÎãàÎã§.

    ÌÜµÍ≥Ñ Í≥ÑÏÇ∞:
        ÏÑ±Í≥µÌïú ÏöîÏ≤≠Îì§Ïóê ÎåÄÌï¥ Îã§Ïùå ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§:
        - ÏßÄÏó∞ÏãúÍ∞Ñ ÌÜµÍ≥Ñ:
            * ÌèâÍ∑†(mean), Ï§ëÏïôÍ∞í(median), ÏµúÏÜå(min), ÏµúÎåÄ(max)
            * Î∞±Î∂ÑÏúÑÏàò: p50, p90, p95, p99
        - Ï≤òÎ¶¨Îüâ ÌÜµÍ≥Ñ:
            * ÌèâÍ∑†, Ï§ëÏïôÍ∞í, ÏµúÏÜå, ÏµúÎåÄ (tokens/s)
        - ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ:
            * ÌèâÍ∑† ÌîÑÎ°¨ÌîÑÌä∏ ÌÜ†ÌÅ∞ Ïàò
            * ÌèâÍ∑† ÏÉùÏÑ± ÌÜ†ÌÅ∞ Ïàò
            * Ï¥ù ÏÉùÏÑ± ÌÜ†ÌÅ∞ Ïàò

    ÌôòÍ≤Ω Î≥ÄÏàò ÏÇ¨Ïö©:
        BENCHMARK_PORT: API ÏÑúÎ≤Ñ Ìè¨Ìä∏ (Í∏∞Î≥∏Í∞í: 8001)
        BENCHMARK_URL: Ï†ÑÏ≤¥ API URL (Ïö∞ÏÑ†ÏàúÏúÑ ÎÜíÏùå)
        BENCHMARK_OUTPUT: Í≤∞Í≥º ÌååÏùºÎ™Ö (Í∏∞Î≥∏Í∞í: results_tensorrt.json)
        BENCHMARK_WARMUP: ÏõåÎ∞çÏóÖ ÏöîÏ≤≠ ÌöüÏàò (Í∏∞Î≥∏Í∞í: 3)

    Ï∂úÎ†• ÌååÏùº ÌòïÏãù:
        JSON ÌòïÏãùÏúºÎ°ú Îã§Ïùå Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìï©ÎãàÎã§:
        {
            "model": "Î™®Îç∏ Í≤ΩÎ°ú",
            "config": {"input_length", "output_length", "temperature"},
            "requests": {"total", "successful", "failed"},
            "latency_seconds": {"mean", "median", "min", "max", "p50", "p90", "p95", "p99"},
            "throughput_tokens_per_sec": {"mean", "median", "min", "max"},
            "tokens": {"avg_prompt_tokens", "avg_completion_tokens", "total_completion_tokens"},
            "raw_results": [Í∞úÎ≥Ñ ÏöîÏ≤≠ Í≤∞Í≥º Î∞∞Ïó¥]
        }

    Returns:
        None: Ìï®ÏàòÎäî Í∞íÏùÑ Î∞òÌôòÌïòÏßÄ ÏïäÏúºÎ©∞, ÏΩòÏÜî Ï∂úÎ†•Í≥º ÌååÏùº Ï†ÄÏû•ÏúºÎ°ú Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.
        Î™®Îì† ÏöîÏ≤≠Ïù¥ Ïã§Ìå®Ìïú Í≤ΩÏö∞ Ïò§Î•ò Î©îÏãúÏßÄÎ•º Ï∂úÎ†•ÌïòÍ≥† Ï°∞Í∏∞ Ï¢ÖÎ£åÌï©ÎãàÎã§.

    ÏòàÏãú:
        >>> # ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï ÌõÑ Ïã§Ìñâ
        >>> import os
        >>> os.environ["BENCHMARK_PORT"] = "8000"
        >>> os.environ["BENCHMARK_OUTPUT"] = "my_results.json"
        >>> main()
        üöÄ Starting TensorRT-LLM Benchmark
        üìä Configuration:
           - API: http://localhost:8000/v1/completions
           - Model: /models/Qwen3-8B
           - Warm-up requests: 3
           - Benchmark requests: 10
        ...
        üíæ Results saved to my_results.json

    Ï∞∏Í≥†:
        - Î™®Îì† ÏöîÏ≤≠Ïù¥ Ïã§Ìå®ÌïòÎ©¥ ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞ÌïòÏßÄ ÏïäÍ≥† Ï¢ÖÎ£åÌï©ÎãàÎã§
        - ÏùºÎ∂Ä ÏöîÏ≤≠Îßå Ïã§Ìå®Ìïú Í≤ΩÏö∞ ÏÑ±Í≥µÌïú ÏöîÏ≤≠Îì§Î°úÎßå ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§
        - Î∞±Î∂ÑÏúÑÏàò Í≥ÑÏÇ∞ Ïãú Ïù∏Îç±Ïä§Í∞Ä Î∞∞Ïó¥ Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇòÏßÄ ÏïäÎèÑÎ°ù int()Î°ú Î≥ÄÌôòÌï©ÎãàÎã§
        - Í≤∞Í≥º ÌååÏùºÏùÄ Í∏∞Ï°¥ ÌååÏùºÏù¥ ÏûàÏúºÎ©¥ ÎçÆÏñ¥ÏîÅÎãàÎã§
    """
    print("üöÄ Starting TensorRT-LLM Benchmark")
    print(f"üìä Configuration:")
    print(f"   - API: {API_URL}")
    print(f"   - Model: {MODEL}")
    print(f"   - Warm-up requests: {NUM_WARMUP}")
    print(f"   - Benchmark requests: {NUM_REQUESTS}")
    print(f"   - Input length: ~{INPUT_LENGTH} tokens")
    print(f"   - Output length: {OUTPUT_LENGTH} tokens")
    print(f"   - Temperature: {TEMPERATURE}")
    print()

    # Warm-up phase
    if NUM_WARMUP > 0:
        print(f"üî• Warm-up phase ({NUM_WARMUP} requests)...")
        warmup_successful = 0
        for i in range(NUM_WARMUP):
            print(f"   Warm-up {i+1}/{NUM_WARMUP}...", end=" ", flush=True)
            result = run_inference(PROMPT, OUTPUT_LENGTH)
            if result["success"]:
                warmup_successful += 1
                print(f"‚úÖ {result['latency']:.2f}s")
            else:
                print(f"‚ùå Failed")
        print(f"‚úÖ Warm-up complete ({warmup_successful}/{NUM_WARMUP} successful)")
        print()

    # Benchmark phase
    print(f"üìä Benchmark phase ({NUM_REQUESTS} requests)...")
    results = []
    successful = 0
    failed = 0

    for i in range(NUM_REQUESTS):
        print(f"üì§ Request {i+1}/{NUM_REQUESTS}...", end=" ", flush=True)
        result = run_inference(PROMPT, OUTPUT_LENGTH)
        results.append(result)

        if result["success"]:
            successful += 1
            print(f"‚úÖ {result['latency']:.2f}s | {result['tokens_per_second']:.2f} tokens/s")
        else:
            failed += 1
            print(f"‚ùå Error: {result.get('error', 'Unknown')}")

    # Calculate statistics
    successful_results = [r for r in results if r["success"]]

    if not successful_results:
        print("\n‚ùå All requests failed!")
        return

    latencies = [r["latency"] for r in successful_results]
    throughputs = [r["tokens_per_second"] for r in successful_results]

    statistics = {
        "model": MODEL,
        "config": {
            "input_length": INPUT_LENGTH,
            "output_length": OUTPUT_LENGTH,
            "temperature": TEMPERATURE
        },
        "requests": {
            "total": NUM_REQUESTS,
            "successful": successful,
            "failed": failed
        },
        "latency_seconds": {
            "mean": mean(latencies),
            "median": median(latencies),
            "min": min(latencies),
            "max": max(latencies),
            "p50": sorted(latencies)[len(latencies)//2],
            "p90": sorted(latencies)[int(len(latencies)*0.9)],
            "p95": sorted(latencies)[int(len(latencies)*0.95)],
            "p99": sorted(latencies)[int(len(latencies)*0.99)]
        },
        "throughput_tokens_per_sec": {
            "mean": mean(throughputs),
            "median": median(throughputs),
            "min": min(throughputs),
            "max": max(throughputs)
        },
        "tokens": {
            "avg_prompt_tokens": int(mean([r["prompt_tokens"] for r in successful_results])),
            "avg_completion_tokens": int(mean([r["completion_tokens"] for r in successful_results])),
            "total_completion_tokens": sum(r["completion_tokens"] for r in successful_results)
        },
        "raw_results": results
    }

    # Print summary
    print("\n" + "="*60)
    print("üìä BENCHMARK RESULTS")
    print("="*60)
    print(f"‚úÖ Successful: {successful}/{NUM_REQUESTS}")
    print(f"‚ùå Failed: {failed}/{NUM_REQUESTS}")
    print()
    print("‚è±Ô∏è  Latency:")
    print(f"   - Mean: {statistics['latency_seconds']['mean']:.3f}s")
    print(f"   - Median: {statistics['latency_seconds']['median']:.3f}s")
    print(f"   - Min: {statistics['latency_seconds']['min']:.3f}s")
    print(f"   - Max: {statistics['latency_seconds']['max']:.3f}s")
    print()
    print("üöÄ Throughput:")
    print(f"   - Mean: {statistics['throughput_tokens_per_sec']['mean']:.2f} tokens/s")
    print(f"   - Median: {statistics['throughput_tokens_per_sec']['median']:.2f} tokens/s")
    print(f"   - Min: {statistics['throughput_tokens_per_sec']['min']:.2f} tokens/s")
    print(f"   - Max: {statistics['throughput_tokens_per_sec']['max']:.2f} tokens/s")
    print()
    print("üìù Tokens:")
    print(f"   - Avg prompt: {statistics['tokens']['avg_prompt_tokens']}")
    print(f"   - Avg completion: {statistics['tokens']['avg_completion_tokens']}")
    print(f"   - Total completion: {statistics['tokens']['total_completion_tokens']}")
    print("="*60)

    # Save results
    output_file = os.environ.get("BENCHMARK_OUTPUT", "results_tensorrt.json")
    with open(output_file, "w") as f:
        json.dump(statistics, f, indent=2)
    print(f"\nüíæ Results saved to {output_file}")

if __name__ == "__main__":
    main()
