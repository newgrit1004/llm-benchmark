[project]
name = "llm-benchmark"
version = "0.1.0"
description = "LLM Inference Benchmark for vLLM and TensorRT-LLM"
authors = [
    {name = "Sewon Kim", email = "flowmad@naver.com"}
]
requires-python = ">=3.12"
dependencies = [
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "ruff>=0.7.4",
    "ty>=0.0.1a1",
    "pytest>=8.3.3",
    "pytest-cov>=6.0.0",
    "pre-commit>=4.0.1",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.ruff]
line-length = 88
target-version = "py312"
fix = true

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
]
ignore = []

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
line-ending = "auto"

[tool.ty]
# Ty configuration (using defaults for preview version)

[tool.pytest.ini_options]
pythonpath = "src"
testpaths = ["tests"]
addopts = [
    "--cov=src",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-fail-under=80",
    "-v"
]
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(message)s"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"

[tool.coverage.run]
source = ["src"]
omit = ["*/tests/*", "*/__pycache__/*", "*/venv/*", "*/.venv/*"]

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
