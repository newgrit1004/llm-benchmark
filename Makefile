DIR?=.

# ============================================
# ê°œë°œ í™˜ê²½ ì„¤ì •
# ============================================

# í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •
.PHONY: setup
setup:
	git config commit.template .gitmessage.txt
	uv sync --all-extras --dev
	uv run pre-commit install

# ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜ (í”„ë¡œë•ì…˜ + ê°œë°œ)
.PHONY: install
install:
	uv sync

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ (lock íŒŒì¼ ê°±ì‹ )
.PHONY: update
update:
	uv lock --upgrade

# ============================================
# ì½”ë“œ í’ˆì§ˆ ê´€ë¦¬
# ============================================

# ì½”ë“œ í¬ë§·íŒ… (Black + isort ëŒ€ì²´)
.PHONY: format
format:
	uv run ruff format ${DIR}

# ì½”ë“œ ë¦°íŒ… ì²´í¬
.PHONY: lint
lint:
	uv run ruff check ${DIR}

# ì½”ë“œ ë¦°íŒ… + ìë™ ìˆ˜ì •
.PHONY: lint-fix
lint-fix:
	uv run ruff check --fix ${DIR}

# íƒ€ì… ì²´í‚¹ (mypy/pyright ëŒ€ì²´)
.PHONY: typecheck
typecheck:
	uv run ty check

# pre-commit ì „ì²´ ì‹¤í–‰
.PHONY: pre-commit
pre-commit:
	uv run pre-commit run --all-files

# ìºì‹œ ë° ì„ì‹œ íŒŒì¼ ì •ë¦¬
.PHONY: clean
clean:
	rm -rf .ruff_cache .pytest_cache .mypy_cache htmlcov .coverage
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete

# ============================================
# LLM ë²¤ì¹˜ë§ˆí¬ ëª…ë ¹ì–´
# ============================================

# ë²¤ì¹˜ë§ˆí¬ìš© ë¡œì»¬ ê°€ìƒí™˜ê²½ ì„¤ì •
.PHONY: setup-benchmark
setup-benchmark:
	@echo "ğŸ”§ ë²¤ì¹˜ë§ˆí¬ìš© ë¡œì»¬ ê°€ìƒí™˜ê²½ ì„¤ì • ì¤‘..."
	@if ! command -v uv &> /dev/null; then \
		echo "âŒ uvê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."; \
		echo "ì„¤ì¹˜: curl -LsSf https://astral.sh/uv/install.sh | sh"; \
		exit 1; \
	fi
	uv venv .venv
	uv pip install --python .venv/bin/python openai python-dotenv
	@echo "âœ… ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì • ì™„ë£Œ!"

# ============================================
# Docker ê´€ë¦¬
# ============================================

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
.PHONY: build-vllm build-tensorrt build-all
build-vllm:
	@echo "ğŸ³ vLLM ë„ì»¤ ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
	cd docker_composes && docker-compose -f docker-compose.vllm.yml build
	@echo "âœ… vLLM ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ!"

build-tensorrt:
	@echo "ğŸ³ TensorRT-LLM ë„ì»¤ ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
	cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml build
	@echo "âœ… TensorRT-LLM ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ!"

build-all: build-vllm build-tensorrt

# Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰/ì¤‘ì§€
.PHONY: up-vllm up-tensorrt down-vllm down-tensorrt
up-vllm:
	@echo "ğŸš€ vLLM ì»¨í…Œì´ë„ˆ ì‹œì‘..."
	cd docker_composes && docker-compose -f docker-compose.vllm.yml up -d
	@echo "âœ… API: http://localhost:8000"

up-tensorrt:
	@echo "ğŸš€ TensorRT-LLM ì»¨í…Œì´ë„ˆ ì‹œì‘..."
	cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml up -d
	@echo "âœ… API: http://localhost:8001"

down-vllm:
	cd docker_composes && docker-compose -f docker-compose.vllm.yml down

down-tensorrt:
	cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml down

# ë¡œê·¸ í™•ì¸
.PHONY: logs-vllm logs-tensorrt
logs-vllm:
	cd docker_composes && docker-compose -f docker-compose.vllm.yml logs -f

logs-tensorrt:
	cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml logs -f

# ============================================
# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================

# ìˆ˜ë™ ë²¤ì¹˜ë§ˆí¬ (ì»¨í…Œì´ë„ˆë¥¼ ì§ì ‘ ì‹œì‘í•œ í›„ ì‹¤í–‰)
.PHONY: benchmark-manual-vllm benchmark-manual-tensorrt
benchmark-manual-vllm:
	@echo "ğŸ“ˆ vLLM ìˆ˜ë™ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘..."
	@echo "âš ï¸  vLLM ì»¨í…Œì´ë„ˆê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (make up-vllm)"
	@BENCHMARK_PORT=8000 BENCHMARK_OUTPUT=results_vllm_manual.json python3 benchmark_llm.py

benchmark-manual-tensorrt:
	@echo "ğŸ“ˆ TensorRT-LLM ìˆ˜ë™ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘..."
	@echo "âš ï¸  TensorRT-LLM ì»¨í…Œì´ë„ˆê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (make up-tensorrt)"
	@BENCHMARK_PORT=8001 BENCHMARK_OUTPUT=results_tensorrt_manual.json python3 benchmark_llm.py

# ìˆœì°¨ì  ìë™ ë²¤ì¹˜ë§ˆí¬ (ì»¨í…Œì´ë„ˆ ìë™ ì‹œì‘/ì¢…ë£Œ)
.PHONY: benchmark benchmark-auto
benchmark: benchmark-auto
benchmark-auto:
	@echo "============================================"
	@echo "ğŸš€ ìë™ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘"
	@echo "============================================"
	@echo ""
	@echo "ğŸ“‹ ì‹¤í–‰ ê³„íš:"
	@echo "  1. vLLM ë²¤ì¹˜ë§ˆí¬ (í¬íŠ¸ 8000)"
	@echo "  2. TensorRT-LLM ë²¤ì¹˜ë§ˆí¬ (í¬íŠ¸ 8001)"
	@echo ""
	@echo "============================================"
	@echo "Phase 1: vLLM ë²¤ì¹˜ë§ˆí¬"
	@echo "============================================"
	@echo "ğŸ³ vLLM ì»¨í…Œì´ë„ˆ ì‹œì‘ ì¤‘..."
	@cd docker_composes && docker-compose -f docker-compose.vllm.yml up -d
	@echo "â³ vLLM ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
	@timeout=300; \
	elapsed=0; \
	while [ $$elapsed -lt $$timeout ]; do \
		if curl -s http://localhost:8000/v1/models > /dev/null 2>&1; then \
			echo "âœ… vLLM ì„œë²„ ì¤€ë¹„ ì™„ë£Œ ($$elapsedì´ˆ)"; \
			break; \
		fi; \
		sleep 5; \
		elapsed=$$((elapsed + 5)); \
		echo "  ëŒ€ê¸° ì¤‘... ($$elapsed/$$timeoutì´ˆ)"; \
	done; \
	if [ $$elapsed -ge $$timeout ]; then \
		echo "âŒ vLLM ì„œë²„ ì‹œì‘ íƒ€ì„ì•„ì›ƒ ($$timeoutì´ˆ ì´ˆê³¼)"; \
		cd docker_composes && docker-compose -f docker-compose.vllm.yml logs --tail=50; \
		cd docker_composes && docker-compose -f docker-compose.vllm.yml down; \
		exit 1; \
	fi
	@echo "ğŸ“ˆ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 8000)..."
	@BENCHMARK_PORT=8000 BENCHMARK_OUTPUT=results_vllm.json python3 benchmark_llm.py || true
	@if [ -f results_vllm.json ]; then \
		echo "âœ… vLLM ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ (results_vllm.json)"; \
	else \
		echo "âŒ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨"; \
	fi
	@echo "ğŸ›‘ vLLM ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì¤‘..."
	@cd docker_composes && docker-compose -f docker-compose.vllm.yml down
	@echo ""
	@echo "============================================"
	@echo "Phase 2: TensorRT-LLM ë²¤ì¹˜ë§ˆí¬"
	@echo "============================================"
	@echo "ğŸ³ TensorRT-LLM ì»¨í…Œì´ë„ˆ ì‹œì‘ ì¤‘..."
	@cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml up -d
	@echo "â³ TensorRT-LLM ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
	@timeout=600; \
	elapsed=0; \
	while [ $$elapsed -lt $$timeout ]; do \
		if curl -s http://localhost:8001/v1/models > /dev/null 2>&1; then \
			echo "âœ… TensorRT-LLM ì„œë²„ ì¤€ë¹„ ì™„ë£Œ ($$elapsedì´ˆ)"; \
			break; \
		fi; \
		sleep 5; \
		elapsed=$$((elapsed + 5)); \
		echo "  ëŒ€ê¸° ì¤‘... ($$elapsed/$$timeoutì´ˆ)"; \
	done; \
	if [ $$elapsed -ge $$timeout ]; then \
		echo "âŒ TensorRT-LLM ì„œë²„ ì‹œì‘ íƒ€ì„ì•„ì›ƒ ($$timeoutì´ˆ ì´ˆê³¼)"; \
		cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml logs --tail=50; \
		cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml down; \
		exit 1; \
	fi
	@echo "ğŸ“ˆ TensorRT-LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 8001)..."
	@BENCHMARK_PORT=8001 BENCHMARK_OUTPUT=results_tensorrt.json python3 benchmark_llm.py || true
	@if [ -f results_tensorrt.json ]; then \
		echo "âœ… TensorRT-LLM ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ (results_tensorrt.json)"; \
	else \
		echo "âŒ TensorRT-LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨"; \
	fi
	@echo "ğŸ›‘ TensorRT-LLM ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì¤‘..."
	@cd docker_composes && docker-compose -f docker-compose.tensorrt-llm.yml down
	@echo ""
	@echo "============================================"
	@echo "âœ… ìë™ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!"
	@echo "============================================"
	@echo "ğŸ“Š ê²°ê³¼ íŒŒì¼:"
	@ls -lh results_*.json 2>/dev/null || echo "  ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
	@echo ""

# ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¹„êµ
.PHONY: compare
compare:
	@echo "ğŸ“Š ê²°ê³¼ ë¹„êµ ì¤‘..."
	@if [ -f results_vllm.json ] && [ -f results_tensorrt.json ]; then \
		echo ""; \
		echo "=== vLLM ê²°ê³¼ ==="; \
		cat results_vllm.json | python3 -m json.tool | grep -A 10 "throughput_tokens_per_sec"; \
		echo ""; \
		echo "=== TensorRT-LLM ê²°ê³¼ ==="; \
		cat results_tensorrt.json | python3 -m json.tool | grep -A 10 "throughput_tokens_per_sec"; \
	else \
		echo "âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'make benchmark'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."; \
	fi

# ============================================
# ëª¨ë¸ ì¤€ë¹„ ì•ˆë‚´
# ============================================
# ëª¨ë¸ì€ HuggingFaceì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤:
#
# ë°©ë²• 1: HuggingFace CLI
#   huggingface-cli download Qwen/Qwen3-8B --local-dir /path/to/models/Qwen3-8B
#
# ë°©ë²• 2: Python
#   from transformers import AutoModelForCausalLM, AutoTokenizer
#   model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B")
#   tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")
#   model.save_pretrained("/path/to/models/Qwen3-8B")
#   tokenizer.save_pretrained("/path/to/models/Qwen3-8B")
